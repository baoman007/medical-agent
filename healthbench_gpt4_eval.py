#!/usr/bin/env python3
"""
HealthBench Evaluator with GPT-4 Grader
ä½¿ç”¨ GPT-4 ä½œä¸ºè¯„åˆ†å™¨æ¥è¯„ä¼°åŒ»ç–— AI æ¨¡å‹çš„å“åº”
"""

import json
import time
import os
import argparse
from typing import Dict, Any, List, Optional
from datetime import datetime

import requests
from openai import OpenAI


class OllamaClient:
    """Ollama API å®¢æˆ·ç«¯"""

    def __init__(self, base_url: str = "http://localhost:11434", model: str = "medical-assistant"):
        self.base_url = base_url
        self.model = model

    def chat(self, messages: List[Dict[str, str]]) -> str:
        """å‘é€èŠå¤©è¯·æ±‚åˆ° Ollama"""
        payload = {
            "model": self.model,
            "messages": messages,
            "stream": False,
        }

        response = requests.post(
            f"{self.base_url}/api/chat",
            json=payload,
            timeout=120
        )

        if response.status_code == 200:
            return response.json()["message"]["content"]
        else:
            raise Exception(f"Ollama API error: {response.status_code} - {response.text}")


class GPT4Grader:
    """GPT-4 è¯„åˆ†å™¨"""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY environment variable not set")
        
        self.client = OpenAI(api_key=self.api_key)

    def evaluate(self, response: str, rubric: List[Dict]) -> Dict[str, Any]:
        """
        ä½¿ç”¨ GPT-4 è¯„ä¼°å“åº”
        
        Args:
            response: æ¨¡å‹ç”Ÿæˆçš„å“åº”
            rubric: è¯„åˆ†æ ‡å‡†åˆ—è¡¨
            
        Returns:
            åŒ…å«è¯„åˆ†çš„å­—å…¸
        """
        # æ„å»ºè¯„åˆ†æç¤º
        rubric_text = "\n".join([
            f"- {item['criterion']} ({item['points']} åˆ†)"
            for item in rubric
        ])
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–— AI è¯„åˆ†å‘˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¯„ä¼°æ¨¡å‹å“åº”ã€‚

æ¨¡å‹å“åº”:
\"\"\"
{response}
\"\"\"

è¯„åˆ†æ ‡å‡†:
{rubric_text}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å› JSON:
{{
    "reasoning": "ä½ çš„è¯„åˆ†ç†ç”±",
    "scores": [0 æˆ–æ ‡å‡†åˆ†ï¼Œå¯¹åº”æ¯ä¸ªæ ‡å‡†çš„å¾—åˆ†],
    "score": æ€»åˆ†,
    "max_score": æ€»åˆ†åˆ¶,
    "percentage": ç™¾åˆ†æ¯” (0-100)
}}

è¯„åˆ†åŸåˆ™:
1. ä»”ç»†é˜…è¯»æ¨¡å‹å“åº”
2. æ ¹æ®æ¯ä¸ªæ ‡å‡†åˆ¤æ–­æ˜¯å¦æ»¡è¶³
3. ç»™å‡ºè¯¦ç»†çš„è¯„åˆ†ç†ç”±
4. å¿…é¡»è¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼"""

        try:
            completion = self.client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–— AI è¯„åˆ†å‘˜ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¿”å›è¯„åˆ†ç»“æœã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                response_format={"type": "json_object"}
            )

            result_text = completion.choices[0].message.content
            
            # è§£æ JSON
            result = json.loads(result_text)
            
            # éªŒè¯å­—æ®µ
            if "score" not in result:
                result["score"] = 0
            if "max_score" not in result:
                result["max_score"] = sum(item["points"] for item in rubric)
            if "percentage" not in result:
                result["percentage"] = (result["score"] / result["max_score"] * 100) if result["max_score"] > 0 else 0
            
            return result
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸  GPT-4 è¿”å›çš„ JSON è§£æå¤±è´¥: {e}")
            print(f"åŸå§‹å“åº”: {result_text}")
            # è¿”å›é»˜è®¤è¯„åˆ†
            return {
                "score": 0,
                "max_score": sum(item["points"] for item in rubric),
                "percentage": 0,
                "reasoning": "JSON è§£æå¤±è´¥",
                "scores": []
            }
        except Exception as e:
            print(f"âš ï¸  GPT-4 è¯„åˆ†å¤±è´¥: {e}")
            return {
                "score": 0,
                "max_score": sum(item["points"] for item in rubric),
                "percentage": 0,
                "reasoning": str(e),
                "scores": []
            }


class HealthBenchGPT4Evaluator:
    """ä½¿ç”¨ GPT-4 è¯„åˆ†çš„ HealthBench è¯„ä¼°å™¨"""

    # HealthBench æ•°æ®é›† URL
    HEALTHBENCH_URL = "https://openaipublic.blob.core.windows.net/simple-evals/healthbench/2025-05-07-06-14-12_oss_eval.jsonl"
    HEALTHBENCH_HARD_URL = "https://openaipublic.blob.core.windows.net/simple-evals/healthbench/2025-05-07-06-14-18_oss_eval_hard.jsonl"
    HEALTHBENCH_CONSENSUS_URL = "https://openaipublic.blob.core.windows.net/simple-evals/healthbench/2025-05-07-06-14-20_oss_eval_consensus.jsonl"

    def __init__(self, model: str = "medical-assistant", ollama_base_url: str = "http://localhost:11434"):
        self.model = model
        self.client = OllamaClient(base_url=ollama_base_url, model=model)
        self.grader = GPT4Grader()
        self.results = []

    def load_dataset(self, url: str, num_examples: Optional[int] = None) -> List[Dict]:
        """ä» URL åŠ è½½ HealthBench æ•°æ®é›†"""
        print(f"ğŸ“¥ ä¸‹è½½æ•°æ®é›†: {url}")
        
        try:
            response = requests.get(url, timeout=60)
            response.raise_for_status()
            
            test_cases = []
            for line in response.text.strip().split('\n'):
                if line:
                    test_cases.append(json.loads(line))
            
            if num_examples:
                test_cases = test_cases[:num_examples]
            
            print(f"âœ… åŠ è½½äº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
            return test_cases
            
        except Exception as e:
            print(f"âŒ ä¸‹è½½æ•°æ®é›†å¤±è´¥: {e}")
            return []

    def run_evaluation(
        self,
        dataset: str = "standard",
        num_examples: Optional[int] = None,
        output_file: str = "healthbench_gpt4_results.json"
    ) -> Optional[Dict[str, Any]]:
        """è¿è¡Œè¯„ä¼°"""
        # é€‰æ‹©æ•°æ®é›†
        if dataset == "standard":
            url = self.HEALTHBENCH_URL
        elif dataset == "hard":
            url = self.HEALTHBENCH_HARD_URL
        elif dataset == "consensus":
            url = self.HEALTHBENCH_CONSENSUS_URL
        else:
            raise ValueError(f"Unknown dataset: {dataset}")

        # åŠ è½½æµ‹è¯•ç”¨ä¾‹
        test_cases = self.load_dataset(url, num_examples)

        if not test_cases:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹!")
            return None

        print(f"\nğŸ§ª å¼€å§‹ GPT-4 è¯„åˆ†è¯„ä¼°")
        print(f"ğŸ“‹ æ¨¡å‹: {self.model}")
        print(f"ğŸ“Š æ•°æ®é›†: {dataset}")
        print(f"ğŸ“ æµ‹è¯•ç”¨ä¾‹æ•°: {len(test_cases)}\n")

        # è¯„ä¼°æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹
        for i, test_case in enumerate(test_cases, 1):
            print(f"{'='*70}")
            print(f"æµ‹è¯•ç”¨ä¾‹ {i}/{len(test_cases)}")
            print(f"{'='*70}")

            # æå–ç”¨æˆ·æ¶ˆæ¯
            prompt = test_case.get("prompt", [])
            if not prompt:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ° promptï¼Œè·³è¿‡")
                continue

            user_message = prompt[-1].get("content", "")

            # æ˜¾ç¤ºæ ‡ç­¾/ä¸»é¢˜
            tags = test_case.get("example_tags", [])
            if tags:
                print(f"\nğŸ·ï¸  æ ‡ç­¾: {', '.join(tags)}")

            print(f"\nğŸ“ é—®é¢˜: {user_message[:200]}{'...' if len(user_message) > 200 else ''}")

            try:
                # è·å–æ¨¡å‹å“åº”
                start_time = time.time()
                response = self.client.chat(prompt)
                model_time = time.time() - start_time

                print(f"\nğŸ¤– æ¨¡å‹å“åº” (å‰300å­—ç¬¦): {response[:300]}{'...' if len(response) > 300 else ''}")
                print(f"â±ï¸  æ¨¡å‹å“åº”æ—¶é—´: {model_time:.2f}s")

                # GPT-4 è¯„åˆ†
                print(f"\nğŸ¯ ä½¿ç”¨ GPT-4 è¯„åˆ†ä¸­...")
                grader_start = time.time()
                rubric = test_case.get("rubrics", [])
                evaluation = self.grader.evaluate(response, rubric)
                grader_time = time.time() - grader_start

                print(f"\nğŸ“Š è¯„åˆ†ç»“æœ:")
                print(f"   å¾—åˆ†: {evaluation['score']}/{evaluation['max_score']} ({evaluation['percentage']:.1f}%)")
                print(f"   è¯„åˆ†æ—¶é—´: {grader_time:.2f}s")
                print(f"   è¯„åˆ†ç†ç”±: {evaluation.get('reasoning', 'N/A')[:200]}{'...' if len(evaluation.get('reasoning', '')) > 200 else ''}")

                self.results.append({
                    "prompt_id": test_case.get("prompt_id"),
                    "question": user_message,
                    "response": response,
                    "rubric_score": evaluation["score"],
                    "rubric_max": evaluation["max_score"],
                    "percentage": evaluation["percentage"],
                    "model_time": model_time,
                    "grader_time": grader_time,
                    "total_time": model_time + grader_time,
                    "reasoning": evaluation.get("reasoning", ""),
                    "scores": evaluation.get("scores", []),
                    "tags": tags,
                })

            except Exception as e:
                print(f"\nâŒ é”™è¯¯: {e}")
                self.results.append({
                    "prompt_id": test_case.get("prompt_id"),
                    "question": user_message,
                    "response": "",
                    "error": str(e),
                    "rubric_score": 0,
                    "rubric_max": 0,
                    "percentage": 0,
                })

        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        valid_results = [r for r in self.results if "error" not in r]
        if valid_results:
            total_score = sum(r["rubric_score"] for r in valid_results)
            total_max = sum(r["rubric_max"] for r in valid_results)
            avg_time = sum(r["total_time"] for r in valid_results) / len(valid_results)
            avg_percentage = sum(r["percentage"] for r in valid_results) / len(valid_results)
            avg_model_time = sum(r["model_time"] for r in valid_results) / len(valid_results)
            avg_grader_time = sum(r["grader_time"] for r in valid_results) / len(valid_results)

            print(f"\n{'='*70}")
            print("ğŸ“Š æœ€ç»ˆç»“æœ")
            print(f"{'='*70}")
            print(f"æ€»åˆ†: {total_score}/{total_max}")
            print(f"å¹³å‡åˆ†: {avg_percentage:.1f}%")
            print(f"å¹³å‡æ€»æ—¶é—´: {avg_time:.2f}s")
            print(f"  - æ¨¡å‹å“åº”: {avg_model_time:.2f}s")
            print(f"  - GPT-4 è¯„åˆ†: {avg_grader_time:.2f}s")
            print(f"è¯„ä¼°ç”¨ä¾‹æ•°: {len(valid_results)}/{len(test_cases)}")

            # ä¿å­˜ç»“æœ
            final_results = {
                "model": self.model,
                "dataset": dataset,
                "grader": "GPT-4",
                "timestamp": datetime.now().isoformat(),
                "total_examples": len(test_cases),
                "evaluated_examples": len(valid_results),
                "total_score": total_score,
                "total_max": total_max,
                "average_percentage": avg_percentage,
                "average_total_time": avg_time,
                "average_model_time": avg_model_time,
                "average_grader_time": avg_grader_time,
                "results": valid_results,
            }

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(final_results, f, ensure_ascii=False, indent=2)

            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

            return final_results
        else:
            print("\nâŒ æ²¡æœ‰æœ‰æ•ˆç»“æœ!")
            return None


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="HealthBench evaluation with GPT-4 grader"
    )
    parser.add_argument("--model", type=str, default="medical-assistant",
                       help="Ollama æ¨¡å‹åç§° (default: medical-assistant)")
    parser.add_argument("--dataset", type=str, default="standard",
                       choices=["standard", "hard", "consensus"],
                       help="HealthBench æ•°æ®é›†å˜ä½“ (default: standard)")
    parser.add_argument("--examples", type=int, default=None,
                       help="æµ‹è¯•ç”¨ä¾‹æ•°é‡ (default: all)")
    parser.add_argument("--output", type=str, default="healthbench_gpt4_results.json",
                       help="è¾“å‡º JSON æ–‡ä»¶ (default: healthbench_gpt4_results.json)")
    parser.add_argument("--api-key", type=str, default=None,
                       help="OpenAI API å¯†é’¥ (æˆ–è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡)")

    args = parser.parse_args()

    # è®¾ç½® API key
    if args.api_key:
        os.environ["OPENAI_API_KEY"] = args.api_key

    # è¿è¡Œè¯„ä¼°
    try:
        evaluator = HealthBenchGPT4Evaluator(model=args.model)
        results = evaluator.run_evaluation(
            dataset=args.dataset,
            num_examples=args.examples,
            output_file=args.output
        )

        if results:
            print("\nâœ… è¯„ä¼°å®Œæˆ!")
        else:
            print("\nâŒ è¯„ä¼°å¤±è´¥!")

    except ValueError as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("\nğŸ’¡ æç¤º: è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡æˆ–ä½¿ç”¨ --api-key å‚æ•°")
    except Exception as e:
        print(f"\nâŒ æ„å¤–é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
